{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:  \n",
    "    #Global variables\n",
    "    input_layer =[]\n",
    "    hidden_layers = []\n",
    "    output_layer = []\n",
    "    output_probs = []\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    best_losses = []\n",
    "    best_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    best_combination = {}\n",
    "    \n",
    "    previousBackpropagatedErrors = []\n",
    "    previousBiasUpdates = []\n",
    "\n",
    "    best_combination_accuracies = []\n",
    "    one_combination_accuracies = []\n",
    "    \n",
    "    batch_size = 200\n",
    "    \n",
    "    #extract data\n",
    "    def loadData(self):\n",
    "        imgs = np.load(\"processed_train_images.npy\", encoding=\"bytes\")\n",
    "        imgs = imgs.reshape(10000, -1)\n",
    "        labels = np.reshape(pd.read_csv(\"train_labels.csv\").values[:, 1], (-1, 1))\n",
    "        return imgs, labels\n",
    "    \n",
    "    #transform the target data into a 1 hot encoded array\n",
    "    def oneHotEncoded(self, target_data):\n",
    "        targets = set(target_data[:, 0])\n",
    "        number_of_outputs = len(set(target_data[:, 0].tolist()))\n",
    "\n",
    "        encoded_targets = np.eye(number_of_outputs)\n",
    "\n",
    "        target_dict = {}\n",
    "        for target, encoded in zip(targets, encoded_targets):\n",
    "            target_dict[target] = encoded\n",
    "\n",
    "        return number_of_outputs, target_dict\n",
    "    \n",
    "    def encodeTargets(self, dict, targets):\n",
    "        targets = targets[:, 0]\n",
    "        encoded_targets = []\n",
    "        for target in targets:\n",
    "            encoded_targets.append(dict.get(target))\n",
    "\n",
    "        encoded_targets = np.array(encoded_targets)\n",
    "        return encoded_targets\n",
    "      \n",
    "    #split the data\n",
    "    def train_test_split(self, X, y, test_size):        \n",
    "        merged = np.append(X, y, axis=1)\n",
    "        #shuffle data\n",
    "        np.random.shuffle(merged)\n",
    "\n",
    "        nbOfRows = len(merged[:, 0])\n",
    "\n",
    "        #split the data randomly\n",
    "        train_data = merged[0: int(nbOfRows * (1 - test_size)), :]\n",
    "        test_data = merged[-int(nbOfRows * test_size):, :]\n",
    "\n",
    "        #split X and Y\n",
    "        y_train = train_data[..., -1:]\n",
    "        X_train = train_data[:, :-1]\n",
    "\n",
    "        y_test = test_data[..., -1:]\n",
    "        X_test = test_data[:, :-1]\n",
    "\n",
    "        #get the target dictionary: convert each target string to a unique one hot encoded array of length 31\n",
    "        number_of_outputs, target_dict = self.oneHotEncoded(y_train)\n",
    "\n",
    "        #convert our targets of string to array of arrays of 31, encoding each string\n",
    "        y_train = self.encodeTargets(target_dict, y_train)\n",
    "        y_test = self.encodeTargets(target_dict, y_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test, number_of_outputs\n",
    "        \n",
    "    #method which does a feedforward on the trained model, and computes the model's accuracy\n",
    "    def testNN(self, X_test, y_test, mode):\n",
    "        first_hidden_layer = self.leaky_relu(np.dot(X_test, self.weights[0]) + self.biases[0])\n",
    "        \n",
    "        weight_nb = len(self.weights) - 1\n",
    "        hidden_layer = first_hidden_layer\n",
    "        \n",
    "        for weight in range(1, weight_nb):\n",
    "            hidden_layer = self.leaky_relu(np.dot(hidden_layer, self.weights[weight]) + self.biases[weight])\n",
    "        \n",
    "        output_layer = self.leaky_relu(np.dot(hidden_layer, self.weights[weight_nb]) + self.biases[weight_nb])\n",
    "\n",
    "        #softmax activation function\n",
    "        probs = self.softmax(output_layer)\n",
    "        \n",
    "        correct = 0\n",
    "        not_correct = 0\n",
    "        for prob, test in zip(probs.tolist(), y_test):\n",
    "            predicted = prob.index(max(prob))\n",
    "            actual = np.nonzero(test)[0][0]\n",
    "\n",
    "            if(predicted ==  actual):\n",
    "                correct += 1\n",
    "            else:\n",
    "                not_correct += 1\n",
    "        \n",
    "        this_accuracy = (correct / (correct + not_correct))\n",
    "        if mode == 'test':\n",
    "            self.test_accuracies.append(this_accuracy)\n",
    "            self.one_combination_accuracies.append(this_accuracy)\n",
    "            print('test accuracy: ', this_accuracy)\n",
    "        else:\n",
    "            self.train_accuracies.append(this_accuracy)\n",
    "            print('train accuracy: ', this_accuracy)\n",
    "        \n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def reportValues(self, epoch):\n",
    "        print('best combination: ', self.best_combination)\n",
    "        print('best accuracy: ', self.best_accuracy)\n",
    "        print('epoch ', epoch)\n",
    "\n",
    "        x_values = np.arange(0, epoch, 1)\n",
    "        y_test_values = self.test_accuracies\n",
    "        y_train_values = self.train_accuracies\n",
    "        shown_x_values = np.arange(0, epoch, 5)\n",
    "\n",
    "        plt.plot(x_values, y_test_values, color='b', label='test')\n",
    "        plt.plot(x_values, y_train_values, color='r', label='train')\n",
    "        plt.xticks(shown_x_values)\n",
    "        plt.ylim(top=0.5)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #this method initializes all the neural network variables.\n",
    "    def initializeParams(self, n_inputs, nodes, m_outputs):   \n",
    "        k = n_inputs\n",
    "        self.biases = []\n",
    "        self.hidden_layers = []\n",
    "        self.weights = []\n",
    "        self.previousBackpropagatedErrors = []\n",
    "        self.previousBiasUpdates = []\n",
    "        for node in nodes: \n",
    "            self.biases.append(np.zeros((1, node)))\n",
    "            self.hidden_layers.append(np.zeros((1, node)))\n",
    "            self.weights.append(np.random.normal(0, 1, [k, node]))\n",
    "            self.previousBackpropagatedErrors.append(np.zeros((k, node)))\n",
    "            self.previousBiasUpdates.append(np.zeros((1, node)))\n",
    "            k = node\n",
    "\n",
    "        self.biases.append(np.zeros((1, m_outputs)))\n",
    "        self.previousBiasUpdates.append(np.zeros((1, m_outputs)))\n",
    "        \n",
    "        self.weights.append(np.random.normal(0, 1, [nodes[len(nodes) - 1], m_outputs]))\n",
    "        self.previousBackpropagatedErrors.append(np.zeros((nodes[len(nodes) - 1], m_outputs)))\n",
    "        \n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        \n",
    "        return self.biases, self.weights\n",
    "    \n",
    "    \n",
    "    def reluDerivative(self, x):\n",
    "        alpha = 0.01\n",
    "        for i in range(0, len(x)):\n",
    "            for k in range(len(x[i])):\n",
    "                if x[i][k] > 0:\n",
    "                    x[i][k] = 1\n",
    "                else:\n",
    "                    x[i][k] = alpha\n",
    "        return x\n",
    "\n",
    "\n",
    "    def leaky_relu(self, x):\n",
    "        alpha = 0.01\n",
    "        for i in range(0, len(x)):\n",
    "            for k in range(0, len(x[i])):\n",
    "                if x[i][k] > 0:\n",
    "                    pass  \n",
    "                else:\n",
    "                    x[i][k] *= alpha\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def tanhActivation(self, data):\n",
    "        return np.tanh(data)\n",
    "    \n",
    "    \n",
    "    def tanhDeriv(self, data):\n",
    "        return 1 - np.tanh(data) ** 2\n",
    "    \n",
    "\n",
    "    def softmax(self, x):\n",
    "        softmax_matrix = np.zeros([x.shape[0], x.shape[1]]).astype(np.float64)\n",
    "        for i, row in enumerate(x):\n",
    "            e_x = np.exp(row - np.max(row))\n",
    "            softmax_matrix [i] = e_x / e_x.sum()\n",
    "        return softmax_matrix\n",
    "    \n",
    "    def lossFunction(self, y_train):\n",
    "        indices = np.argmax(y_train, axis = 1).astype(int)\n",
    "        predicted_probability = self.output_probs[np.arange(len(self.output_probs)), indices]\n",
    "        log_preds = np.log(predicted_probability)\n",
    "        loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "        return loss\n",
    "\n",
    "    #this method computes the average accuracies of each run of the cross validation, and stores it.\n",
    "    def storeAverageAccuracies(self):\n",
    "        nb_accuracies = len(self.one_combination_accuracies)\n",
    "        sum = 0\n",
    "        for accuracy in self.one_combination_accuracies:\n",
    "            sum += accuracy\n",
    "        \n",
    "        avg = sum / nb_accuracies\n",
    "        self.best_combination_accuracies.append(avg)\n",
    "        self.one_combination_accuracies = []\n",
    "                \n",
    "    #this method gets the index of the best combination of the cross validaton. \n",
    "    def storeBestCombination(self, combinations):\n",
    "        best_acc = max(self.best_combination_accuracies)\n",
    "        print('bestacc ', best_acc)\n",
    "        best_index = self.best_combination_accuracies.index(best_acc)\n",
    "        print('best index ', best_index)\n",
    "        self.best_combination = combinations[best_index]\n",
    "        print('best combiantion: ', self.best_combination)\n",
    "        print('=========================================')\n",
    "\n",
    "        print('best avg accuracies: ')\n",
    "        print(self.best_combination_accuracies)\n",
    "\n",
    "        print('combinations: ')\n",
    "        print(combinations)\n",
    "\n",
    "\n",
    "    #check if the input layer is the right one!!!!!\n",
    "    def feedForward(self, X_train):\n",
    "        #compute the input, hidden and output layers  \n",
    "        self.input_layer = X_train\n",
    "        \n",
    "        #feedforward on the first hidden layer, we always save the input and output of each hidden layer\n",
    "        first_hidden_layer = []\n",
    "        first_hidden_layer.append(np.dot(X_train, self.weights[0]) + self.biases[0])\n",
    "        first_hidden_layer.append(self.leaky_relu(first_hidden_layer[0]))\n",
    "        \n",
    "        nb_hidden_layers = len(self.weights) - 1\n",
    "        hidden_layer = first_hidden_layer\n",
    "        \n",
    "        #update the hidden layers array.\n",
    "        self.hidden_layers[0] = hidden_layer\n",
    "\n",
    "        #for each hidden layer in the network\n",
    "        for layer_nb in range(1, nb_hidden_layers):\n",
    "            #compute both the input and output of each hidden layer, and then store it\n",
    "            temp_hidden_layer = []\n",
    "            temp_hidden_layer.append(np.dot(hidden_layer[1], self.weights[layer_nb]) + self.biases[layer_nb])\n",
    "            temp_hidden_layer.append(self.leaky_relu(temp_hidden_layer[0]))\n",
    "            hidden_layer = temp_hidden_layer\n",
    "            self.hidden_layers[layer_nb] = hidden_layer\n",
    "        \n",
    "        #compute the input and output of the output layer\n",
    "        output = []\n",
    "        output.append(np.dot(hidden_layer[1], self.weights[nb_hidden_layers]) + self.biases[nb_hidden_layers])\n",
    "        output.append(self.leaky_relu(output[0]))\n",
    "        self.output_layer = output\n",
    "        \n",
    "        #softmax activation function to get the probabilities\n",
    "        self.output_probs = self.softmax(self.output_layer[1])\n",
    "           \n",
    "    \n",
    "    def backProp(self, alpha, momentum, y_train):\n",
    "        #compute the error rate    \n",
    "        backPropagatedErrors = []\n",
    "        output_error = (self.output_probs - y_train)\n",
    "        error = np.sum(output_error**2)\n",
    "        \n",
    "        #compute the derivative of the output function, and then the first backpropagated error, and store it\n",
    "        output_derivatives = self.reluDerivative(self.output_layer[1])\n",
    "        backPropagatedErrors.append(np.multiply(output_error, output_derivatives))\n",
    "        \n",
    "        number_hidden = len(self.hidden_layers)\n",
    "        number_weights = len(self.weights) - 1\n",
    "\n",
    "        #for each hidden layer, compute the backpropagated error\n",
    "        for hidden_nb, propagatedError in zip(range(number_hidden - 1, -1, -1), range(0, number_hidden)):\n",
    "            hidden_layer = self.hidden_layers[hidden_nb]\n",
    "            weight = self.weights[number_weights]\n",
    "            last_error = backPropagatedErrors[propagatedError]\n",
    "            next_error = np.multiply(np.dot(last_error, weight.T), self.reluDerivative(hidden_layer[1]))\n",
    "            backPropagatedErrors.append(next_error)\n",
    "            number_weights -= 1\n",
    "\n",
    "        \n",
    "        #compute the gradient weight updates, with momentum. the previously backpropagated errors refer too the backpropagatederror of the last batch\n",
    "        dW0 = np.dot(self.input_layer.T, backPropagatedErrors[len(backPropagatedErrors) - 1]) + momentum * self.previousBackpropagatedErrors[0]\n",
    "        #update the first weight\n",
    "        self.weights[0] = self.weights[0] - alpha * dW0\n",
    "        #save the update, so we use it in the next backpropagation as momentum\n",
    "        self.previousBackpropagatedErrors[0] = dW0\n",
    "        \n",
    "        #compute the first gradient bias update, with momentum\n",
    "        db0 = np.sum(backPropagatedErrors[len(backPropagatedErrors) - 1], axis = 0, keepdims = True) + momentum * self.previousBiasUpdates[0]\n",
    "        #update the bias\n",
    "        self.biases[0] = self.biases[0] - alpha * db0\n",
    "        #store it so we can use it in the next backpropagation as momentum\n",
    "        self.previousBiasUpdates[0] = db0\n",
    "        \n",
    "        #update each hidden layer weight and bias, with momentum as well. \n",
    "        layer_nb = 0\n",
    "        for index, error in zip(range(1, len(self.weights)), range(len(backPropagatedErrors) - 2, -1, -1)):\n",
    "            dW = np.dot(self.hidden_layers[layer_nb][1].T, backPropagatedErrors[error]) + momentum * self.previousBackpropagatedErrors[index]\n",
    "            db = np.sum(backPropagatedErrors[error], axis = 0, keepdims = True) + momentum * self.previousBiasUpdates[index]\n",
    "            \n",
    "            self.weights[index] = self.weights[index] - alpha * dW\n",
    "            self.biases[index] = self.biases[index] - alpha * db\n",
    "            self.previousBackpropagatedErrors[index] = dW\n",
    "            self.previousBiasUpdates[index] = db\n",
    "            layer_nb += 1\n",
    "        \n",
    "        \n",
    "\n",
    "    #backpropagation algorithm\n",
    "    def backPropagation(self, X_train, X_test, y_train, y_test, number_of_outputs, biases, weights, combination, epoch, nodes, alpha, momentum):    \n",
    "        #for each epoch, and for each mini-batch, call do a feedforward followed by a backprop\n",
    "        for times in range(0, epoch):\n",
    "\n",
    "            for batch in range(0, int(X_train.shape[0] / self.batch_size)):            \n",
    "                start = batch * self.batch_size\n",
    "                end = (batch + 1) * self.batch_size\n",
    "                \n",
    "                train_batch = X_train[start : end]\n",
    "                target_batch = y_train[start: end]\n",
    "                \n",
    "                self.feedForward(train_batch)\n",
    "                self.backProp(alpha, momentum, target_batch)\n",
    "\n",
    "            self.testNN(X_test, y_test, 'test')\n",
    "            self.testNN(X_train, y_train, 'train') \n",
    "    \n",
    "    def neuralNetwork(self, imgs, labels):\n",
    "        #TODO change them before submitting hyperparameters\n",
    "        epoch = [50]\n",
    "        alpha = [0.00001, 0.0001, 0.001]\n",
    "        nodes = [[2048], [2048, 1024], [2048, 1024, 512], [2048, 1024, 512, 512]]\n",
    "        momentum = [0.00001, 0.0005, 0.0001, 0.001]\n",
    "\n",
    "        cross_validation = 5\n",
    "        hyperparameters_list = ['epoch', 'nodes', 'alpha', 'momentum']\n",
    "        hyperparameters = [epoch, nodes, alpha, momentum]   \n",
    "\n",
    "        #create the combinations of hyperparameters\n",
    "        combinations = list(itertools.product(*hyperparameters))\n",
    "\n",
    "        #for each combination, do a cross validation \n",
    "        for combination in combinations:\n",
    "            print('combination: ', combination)\n",
    "            args = {}\n",
    "            for hyperparam, param_val in zip(hyperparameters_list, combination):\n",
    "                args.update({hyperparam:param_val})\n",
    "            #cross validation: split the data 80 20, initialize the neural netowrk parameters, run the backpropagation algorithm, test each run, and then take the avg of the 5 runs\n",
    "            for index in range(0, cross_validation):\n",
    "                #train-test split the data\n",
    "                X_train, X_test, y_train, y_test, number_of_outputs = NN.train_test_split(imgs, labels, 0.2)\n",
    "                X_train = np.array(X_train, dtype=np.float64)\n",
    "                X_test = np.array(X_test, dtype=np.float64)\n",
    "\n",
    "                #initialize the Neural Network's parameters\n",
    "                biases, weights = self.initializeParams(len(X_train[0, :]), combination[1], number_of_outputs)\n",
    "                #train the neural network\n",
    "                self.backPropagation(X_train, X_test, y_train, y_test, number_of_outputs, biases, weights, combination, **args)\n",
    "                #test the neural network\n",
    "                \n",
    "            #compute the avg of the 5 runs, and store it\n",
    "            self.storeAverageAccuracies()\n",
    "        \n",
    "        #get the best combination of parameters\n",
    "        self.storeBestCombination(combinations)\n",
    "        #visualize the best combination\n",
    "        self.reportValues(combination[0])\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "imgs, labels = NN.loadData()\n",
    "NN.neuralNetwork(imgs, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
